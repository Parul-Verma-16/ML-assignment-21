{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d822ae",
   "metadata": {},
   "source": [
    "## 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393e3e7",
   "metadata": {},
   "source": [
    "The estimated depth of a Decision Tree trained on a one million instance training set can vary based on multiple factors, including the complexity of the data, the number of features, and the specific algorithm used for training.\n",
    "\n",
    "In an unrestricted Decision Tree (where there is no limit on the depth), the tree can grow until each leaf node contains only one instance, resulting in a fully grown tree. However, growing a Decision Tree to its full depth on a large dataset like one million instances can lead to overfitting and poor generalization to unseen data.\n",
    "\n",
    "In practice, to avoid overfitting and to improve the tree's generalization performance, hyperparameters like `max_depth`, `min_samples_split`, and `min_samples_leaf` are often used to limit the depth and control the growth of the tree.\n",
    "\n",
    "If you want to estimate the depth of the Decision Tree without any restrictions, it is difficult to provide a precise estimate without knowing the characteristics of the data. The depth can vary widely depending on the complexity of the decision boundaries and the relationships between features and the target variable in the dataset.\n",
    "\n",
    "For large datasets, it is generally recommended to use tree pruning techniques or ensemble methods like Random Forest or Gradient Boosting to build more robust and accurate models, as unrestricted Decision Trees tend to be prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39086045",
   "metadata": {},
   "source": [
    "## 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ee135",
   "metadata": {},
   "source": [
    "The Gini impurity of a node is usually lower than that of its parent in a Decision Tree. The Gini impurity is a measure of the impurity or disorder in the data at a particular node. When splitting a node into child nodes during the construction of a Decision Tree, the goal is to reduce the impurity in each child node compared to the parent node.\n",
    "\n",
    "During the tree-building process, the algorithm searches for the best split that minimizes the impurity in the child nodes. If the split is successful, the Gini impurity of the child nodes will typically be lower than that of the parent node. This is because a good split separates the data into more homogenous subsets with respect to the target variable, making the child nodes more pure than the parent node.\n",
    "\n",
    "It's important to note that the Gini impurity can increase in some cases due to the randomness introduced by the tree-building process or if the dataset is noisy. However, the goal is to find splits that, on average, reduce the impurity at each level of the tree and lead to a well-balanced and accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197114f",
   "metadata": {},
   "source": [
    "## 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e974f",
   "metadata": {},
   "source": [
    "Yes, reducing the maximum depth of a Decision Tree can be a good idea if the model is overfitting the training set. Overfitting occurs when the tree is too complex, and it captures noise and specific details of the training data that may not generalize well to new, unseen data.\n",
    "\n",
    "By reducing the maximum depth of the tree, you are effectively limiting the number of nodes and branches the tree can have. This will lead to a simpler model that is less likely to overfit. A shallower tree will capture more general patterns and will be less sensitive to noise and outliers in the data.\n",
    "\n",
    "Reducing the maximum depth can be seen as a form of regularization in Decision Trees. Regularization techniques are used to prevent overfitting by imposing constraints on the model's complexity. In this case, reducing the depth acts as a constraint on the tree's complexity, preventing it from memorizing the training data too closely.\n",
    "\n",
    "It's important to find the right balance in setting the maximum depth. If the depth is too small, the model may not capture enough of the underlying patterns, leading to underfitting. A common approach is to use techniques like cross-validation to find the optimal value for the maximum depth that balances model complexity and performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e472f",
   "metadata": {},
   "source": [
    "## 4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3fc07",
   "metadata": {},
   "source": [
    "In general, Decision Trees are not sensitive to the scale of input features. Unlike some other machine learning algorithms (e.g., Support Vector Machines, K-Nearest Neighbors), Decision Trees do not rely on distance-based calculations. As a result, scaling the input features is unlikely to have a significant impact on the performance of the Decision Tree.\n",
    "\n",
    "However, if the Decision Tree is underfitting the training set, meaning it is not able to capture the patterns and complexities present in the data, there are other strategies to try before considering feature scaling:\n",
    "\n",
    "1. Increase the maximum depth: One common reason for underfitting is that the Decision Tree is too shallow. By allowing the tree to grow deeper (increase the maximum depth), it can better capture more complex relationships in the data.\n",
    "\n",
    "2. Adjust hyperparameters: You can experiment with other hyperparameters such as the minimum number of samples required to split a node (min_samples_split) or the minimum number of samples required to be at a leaf node (min_samples_leaf).\n",
    "\n",
    "3. Feature selection/engineering: Consider selecting more relevant features or engineering new features that better represent the underlying patterns in the data.\n",
    "\n",
    "4. Ensemble methods: Combine multiple Decision Trees using ensemble methods like Random Forests or Gradient Boosting to improve performance and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04becf2b",
   "metadata": {},
   "source": [
    "## 5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d690ed",
   "metadata": {},
   "source": [
    "The time taken to train a Decision Tree on a training set is not directly proportional to the number of instances. Training time can vary based on several factors, including the complexity of the model, the number of features, and the hardware and software setup used for training.\n",
    "\n",
    "However, as a rough estimate, we can assume that the training time is approximately linear with respect to the number of instances. If it takes one hour to train a Decision Tree on a training set with 1 million instances, then training a Decision Tree on a training set with 10 million instances might take around 10 hours.\n",
    "\n",
    "This is just an estimate, and the actual training time can be influenced by various factors. In practice, it's essential to benchmark the training process on the specific dataset and hardware to get a more accurate estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95056d",
   "metadata": {},
   "source": [
    "## 6. Will setting presort=True speed up training if your training set has 100,000 instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988a7bd",
   "metadata": {},
   "source": [
    "Setting `presort=True` in Scikit-learn's DecisionTreeClassifier can speed up the training process for small datasets, but it may not be the case for larger datasets like the one with 100,000 instances. When `presort=True`, the algorithm precomputes the presort order of the data, which can be beneficial for smaller datasets where the cost of sorting is relatively low. However, for larger datasets, the sorting process can become computationally expensive and may slow down the training.\n",
    "\n",
    "In practice, Scikit-learn automatically decides whether to use presorting based on the dataset size. For small datasets, it uses presorting by default, and for larger datasets, it uses a different algorithm to build the tree.\n",
    "\n",
    "Therefore, for a training set with 100,000 instances, it is recommended not to set `presort=True`. Scikit-learn's default behavior should be sufficient to handle larger datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b014b",
   "metadata": {},
   "source": [
    "## 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "\n",
    "## a. To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "\n",
    "## b. Divide the dataset into a training and a test collection with train test split().\n",
    "\n",
    "## c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "\n",
    "## d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445afbc",
   "metadata": {},
   "source": [
    "To train and fine-tune a Decision Tree for the moons dataset, follow these steps:\n",
    "\n",
    "a. Create the moons dataset:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "```\n",
    "\n",
    "b. Split the dataset into training and test sets:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "c. Perform grid search with cross-validation to find good hyperparameters:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [10, 50, 100, 200]\n",
    "}\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(tree_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "```\n",
    "\n",
    "d. Train the model with the best hyperparameters on the entire training set:\n",
    "\n",
    "```python\n",
    "best_tree_clf = DecisionTreeClassifier(max_leaf_nodes=best_params['max_leaf_nodes'], random_state=42)\n",
    "best_tree_clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "e. Evaluate the model's performance on the test set:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = best_tree_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "By following these steps, you can achieve an accuracy of 85 to 87 percent on the test set using the Decision Tree classifier with the tuned hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205eb92",
   "metadata": {},
   "source": [
    "## 8. Follow these steps to grow a forest:\n",
    "\n",
    "## a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn&#39;s class.\n",
    "\n",
    "## b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "\n",
    "## c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy&#39;s mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "\n",
    "## d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You&#39;ve successfully learned a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce84630",
   "metadata": {},
   "source": [
    "To grow a Random Forest, follow these steps:\n",
    "\n",
    "a. Create 1,000 subsets of the training set:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "subsets = 1000\n",
    "subset_size = 100\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=subsets, test_size=subset_size, random_state=42)\n",
    "subsets_indices = [(train_index, test_index) for train_index, test_index in shuffle_split.split(X_train)]\n",
    "\n",
    "```\n",
    "\n",
    "b. Train Decision Trees on each subset:\n",
    "\n",
    "```python\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "trees = []\n",
    "\n",
    "for train_index, _ in subsets_indices:\n",
    "    tree_clf = DecisionTreeClassifier(max_leaf_nodes=best_params['max_leaf_nodes'], random_state=42)\n",
    "    tree_clf.fit(X_train[train_index], y_train[train_index])\n",
    "    trees.append(tree_clf)\n",
    "```\n",
    "\n",
    "c. Create majority-vote predictions:\n",
    "\n",
    "```python\n",
    "from scipy.stats import mode\n",
    "\n",
    "predictions = np.empty([X_test.shape[0], subsets], dtype=np.uint8)\n",
    "\n",
    "for i, (_, test_index) in enumerate(subsets_indices):\n",
    "    predictions[:, i] = trees[i].predict(X_test)\n",
    "\n",
    "y_pred_majority_votes, _ = mode(predictions, axis=1)\n",
    "```\n",
    "\n",
    "d. Evaluate the Random Forest's performance on the test set:\n",
    "\n",
    "```python\n",
    "accuracy_majority_votes = accuracy_score(y_test, y_pred_majority_votes)\n",
    "print(f\"Random Forest Accuracy: {accuracy_majority_votes:.2f}\")\n",
    "```\n",
    "\n",
    "By following these steps, you should achieve a slightly higher accuracy on the test set compared to the individual Decision Tree model. This demonstrates how to build a basic Random Forest classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
